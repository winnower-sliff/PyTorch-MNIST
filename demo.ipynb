{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2def2935-8374-4632-a691-5cc462082fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set...\n",
      "Using  cpu\n",
      "\n",
      "-----------------\n",
      "Num of epoch: 40\n",
      "Batch size: 10\n",
      "Num of batch: 0\n",
      "-----------------\n",
      "\n",
      "Start training...\n",
      "Training epoch 1/40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 这一步将给出每张图片的分类结果，BATCH_SIZE*1\u001b[39;00m\n\u001b[0;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 81\u001b[0m loss \u001b[38;5;241m=\u001b[39m cost(outputs, labels)\n\u001b[0;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     83\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1189\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1190\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (10)."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cnn import wt_net\n",
    "\n",
    "\n",
    "DATA_PATH = \"./DataSets\"\n",
    "MODEL_PATH = \"./Models\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 40\n",
    "\n",
    "print(\"Loading train set...\")\n",
    "# 打开 CSV 文件\n",
    "data = pd.read_csv(r\".\\DataSets\\SLF\\datas.csv\", header=None)\n",
    "train_set = []\n",
    "\n",
    "# 将数据重塑为样本和特征的形式\n",
    "# 每个样本由2行组成，因此总共有500个样本\n",
    "num_samples = 500\n",
    "num_features = 4070\n",
    "samples = data.values.reshape(num_samples, 1, 2, num_features)\n",
    "labels = torch.ones(num_samples, dtype=torch.long)  # 每个样本的标签都是1\n",
    "\n",
    "\n",
    "# 创建一个自定义的Dataset类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx], dtype=torch.float32)\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "# 创建Dataset实例\n",
    "dataset = CustomDataset(samples, labels)\n",
    "\n",
    "# 创建DataLoader实例\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "BATCH_NUM = math.ceil(len(train_set) / BATCH_SIZE)\n",
    "print(\"Using \", DEVICE)\n",
    "\n",
    "# 建立模型并载入设备\n",
    "model = wt_net.MyCNN().to(DEVICE)\n",
    "# 定义损失及优化器\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(\n",
    "    \"\\n-----------------\\n\"\n",
    "    \"Num of epoch: {}\\n\"\n",
    "    \"Batch size: {}\\n\"\n",
    "    \"Num of batch: {}\".format(EPOCH, BATCH_SIZE, BATCH_NUM)\n",
    ")\n",
    "print(\"-----------------\\n\")\n",
    "print(\"Start training...\")\n",
    "# 训练\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"Training epoch {}/{}\".format(epoch + 1, EPOCH))\n",
    "    num_correct = 0\n",
    "    val_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        num_correct_batch = 0\n",
    "        val_loss_batch = 0\n",
    "        # 注意这里的images和labels均为一个batch的图片和标签\n",
    "        images = images.to(DEVICE).float()  # BATCH_SIZE*28*28\n",
    "        labels = labels.to(DEVICE)  # BATCH_SIZE*1\n",
    "\n",
    "        outputs = model(images)\n",
    "        pred = torch.max(outputs, 1)[1]  # 这一步将给出每张图片的分类结果，BATCH_SIZE*1\n",
    "        optimizer.zero_grad()\n",
    "        loss = cost(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        val_loss_batch += loss.data\n",
    "        val_loss += val_loss_batch\n",
    "        num_correct_batch += (pred == labels).sum().item()\n",
    "        num_correct += num_correct_batch\n",
    "        print(\n",
    "            \"Batch {}/{}, Loss: {:.6f}, Accuracy: {:.6f}%\".format(\n",
    "                batch_idx + 1,\n",
    "                BATCH_NUM,\n",
    "                val_loss_batch / BATCH_SIZE,\n",
    "                100 * num_correct_batch / BATCH_SIZE,\n",
    "            )\n",
    "        )\n",
    "    print(\n",
    "        \"Epoch {}: Loss: {:.6f}, Accuracy: {:.6f}%\\n\".format(\n",
    "            epoch + 1, val_loss / len(train_set), 100 * num_correct / len(train_set)\n",
    "        )\n",
    "    )\n",
    "# 保存整个网络\n",
    "print(\"Saving the model...\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "torch.save(model, MODEL_PATH + \"/MyCNN_MNIST.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f2e208-2f40-4d41-a0cc-08ea370da6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(len(dataset) / BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
